Core Gen-AI Models:

-   Variational Autoencoders (VAE)
-   Generative Adversarial Networks (GAN)
-   Transformer-based models (E.g. Generative Pretrained Transformer GPT)
-   Diffusion Models

Variational Autoencoders (VAEs):
Work with diverse data types (images, text, audio).
Reduce dimensionality to create improved versions of data.
Use an encoder to compress data into a latent space and a decoder to reconstruct it.
Applications include image synthesis, data compression, and anomaly detection.

Generative Adversarial Networks (GANs):
Consist of two competing neural networks: a generator and a discriminator.
The generator creates data samples, while the discriminator evaluates their authenticity.
Used for generating realistic images, style transfer, and deep fakes.
Require significant data and computational power.

Transformer-based Models:
Address limitations of recurrent neural networks (RNNs) in processing long text sequences.
Utilize attention mechanisms to focus on important text parts.
Capable of natural language processing and content generation.

Diffusion Models:
Recent models that mitigate data decay due to noise.
Use a two-step process: forward diffusion (adding noise) and reverse diffusion (recovering data).
Known for generating high-quality images and videos.

Foundation Models:

A foundation model is a large, general-purpose AI model trained on vast amounts of unlabeled data, allowing it to adapt to various applications.

    Characteristics:
    Pre-training: Foundation models undergo pre-training using unsupervised algorithms, developing multimodal capabilities (text, image, audio, video).

    Generative AI: While all foundation models have generative capabilities, not all generative AI models qualify as foundation models.

    Examples:
    Large Language Models (LLMs): Models like OpenAI's GPT-3 and GPT-4, which are pre-trained on billions of parameters, enabling them to perform complex tasks.

    Image Generation: Tools like Dall-E, which generate images from text prompts.

    Adaptability:
    Foundation models can be customized for specific applications, making AI more accessible to businesses.

    Limitations:
    Potential biases in training data and the risk of generating inaccurate information (hallucination).
